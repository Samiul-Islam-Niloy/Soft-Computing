{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_170104016.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h42yKWQd64jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TsyEQH-MHwG"
      },
      "source": [
        "!pip install torchmetrics\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torchmetrics\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/Computer Science and Engineering (CSE)/4th Year 2nd Semester/CSE4238 Soft Computing Lab/Assignment 01/imagery/'"
      ],
      "metadata": {
        "id": "aAUZAdlXQY1Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "                  transforms.Grayscale(num_output_channels=1),\n",
        "                  transforms.Resize((32, 32)), \n",
        "                  transforms.ToTensor(),])"
      ],
      "metadata": {
        "id": "DpJyKueb62kb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(data_dir, transform=train_transform)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "12Q2UvJwQIJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 50\n",
        "torch.manual_seed(random_seed);\n",
        "test_size = math.floor(0.2 * len(dataset))\n",
        "val_size = math.floor(0.15 * len(dataset))\n",
        "train_size = len(dataset) - val_size - test_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "rpm1MZCOQ7zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "  metric = torchmetrics.Accuracy()\n",
        "  metric = metric.cuda()\n",
        "  return metric(outputs, labels)\n",
        "\n",
        "def precision(outputs, labels):\n",
        "  metric = torchmetrics.Precision(average='macro', num_classes=4)\n",
        "  metric = metric.cuda()\n",
        "  return metric(outputs, labels)\n",
        "\n",
        "def recall(outputs, labels):\n",
        "  metric = torchmetrics.Recall(average='macro', num_classes=4)\n",
        "  metric = metric.cuda()\n",
        "  return metric(outputs, labels)\n",
        "\n",
        "def f1_score(outputs, labels):\n",
        "  pr = precision(outputs, labels)\n",
        "  re = recall(outputs, labels)\n",
        "  return 2 * pr * re / (pr + re)"
      ],
      "metadata": {
        "id": "fi3CwhEhR4WX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class NNModel(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, out_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear5 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear6 = nn.Linear(hidden_size, out_size)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten the image tensors\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        # Get intermediate outputs using hidden layer\n",
        "        out = self.linear1(xb)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear3(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear4(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.linear5(out)\n",
        "        out = F.relu(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear6(out)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        prec = precision(out, labels)\n",
        "        rec = recall(out, labels)\n",
        "        f1 = f1_score(out, labels)\n",
        "        return {'val_loss':loss, 'val_acc': acc, 'val_recall': rec, 'val_precision': prec, 'val_f1': f1}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()  \n",
        "        batch_recall = [x['val_recall'] for x in outputs]\n",
        "        epoch_recall = torch.stack(batch_recall).mean()  \n",
        "        batch_precision = [x['val_precision'] for x in outputs]\n",
        "        epoch_precision = torch.stack(batch_precision).mean()  \n",
        "        batch_f1 = [x['val_f1'] for x in outputs]\n",
        "        epoch_f1 = torch.stack(batch_f1).mean()     \n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'val_recall': epoch_recall.item(), 'val_precision': epoch_precision.item(), 'val_f1': epoch_f1.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}, val_recall: {:.4f}, val_precision: {:.4f}, val_f1: {:.4f}\".format(epoch, result['val_loss'], result['val_acc'], result['val_recall'], result['val_precision'], result['val_f1']))"
      ],
      "metadata": {
        "id": "Tq3UT6WTRPXH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 32*32\n",
        "hidden_size = 100\n",
        "num_classes = 4"
      ],
      "metadata": {
        "id": "_N8OEwwhRWli"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "VHrzdYCDRaYW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "id": "YClNPfbfRcLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(val_dataset, batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size)\n",
        "model = NNModel(input_size, hidden_size, out_size=num_classes)\n",
        "\n",
        "# Moving the model and datasets to default device (gpu if available)\n",
        "to_device(model, device)\n",
        "train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
        "validation_dataloader = DeviceDataLoader(validation_dataloader, device)\n",
        "test_dataloader = DeviceDataLoader(test_dataloader, device)"
      ],
      "metadata": {
        "id": "qJnPSG6XvEwv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "metadata": {
        "id": "JAqe2nAtRfjW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model (on GPU)\n",
        "model = NNModel(input_size, hidden_size, out_size=num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "6H6vrlJURoRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 200\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "6T0eNBZyOFJ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit(num_epochs, lr, model, train_dataloader, validation_dataloader)"
      ],
      "metadata": {
        "id": "6BFwpxx5RqDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_accuracies(history, metric, ylabel, title):\n",
        "    accuracies = [x[metric] for x in history]\n",
        "    plt.plot(accuracies)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title);"
      ],
      "metadata": {
        "id": "fuNaxDjnRs0O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-b')\n",
        "    plt.plot(val_losses, '-r')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');"
      ],
      "metadata": {
        "id": "OG-6dCMGcmkA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(history)"
      ],
      "metadata": {
        "id": "dFr2NLrlc5RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracies(history, 'val_acc', 'Validation Accuracy', 'Validation Accuracy vs. No. of epochs')"
      ],
      "metadata": {
        "id": "3QBDe5g0Rt61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracies(history, 'val_f1', 'Validation F1 Score', 'Validation F1 Score vs. No. of epochs')"
      ],
      "metadata": {
        "id": "AOgdYQiDbOUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the test dataset performance\n",
        "test_history = [evaluate(model, test_dataloader)]"
      ],
      "metadata": {
        "id": "JVz10n_tRrvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_history"
      ],
      "metadata": {
        "id": "3OOX0NZAczAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/Computer Science and Engineering (CSE)/4th Year 2nd Semester/CSE4238 Soft Computing Lab/Assignment 01/one/nnmodel.pth')"
      ],
      "metadata": {
        "id": "HLDoe5jwgb9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 02**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p1NUTt-rQLwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping num_output_channels=3, will get better performance with CNN Model\n",
        "train_transform_cnn = transforms.Compose([\n",
        "                      transforms.Resize((32, 32)), \n",
        "                      transforms.RandomRotation(20),\n",
        "                      transforms.ToTensor(),])"
      ],
      "metadata": {
        "id": "LtW-rR9s7vqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(data_dir, transform=train_transform_cnn)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "YoHsAhKqd9Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 50\n",
        "torch.manual_seed(random_seed);\n",
        "test_size = math.floor(0.2 * len(dataset))\n",
        "val_size = math.floor(0.15 * len(dataset))\n",
        "train_size = len(dataset) - val_size - test_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "fxLCFWwdd9Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKnenzvbgzN-"
      },
      "source": [
        "batch_size =20\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(val_dataset, batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi_Zvy71i6sU"
      },
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        out = out.to(device='cuda')\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        labels = labels.to(device='cuda')\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        prec = precision(out, labels)\n",
        "        rec = recall(out, labels)\n",
        "        f1 = f1_score(out, labels)\n",
        "        return {'val_loss':loss, 'val_acc': acc, 'val_recall': rec, 'val_precision': prec, 'val_f1': f1}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        batch_recall = [x['val_recall'] for x in outputs]\n",
        "        epoch_recall = torch.stack(batch_recall).mean()  \n",
        "        batch_precision = [x['val_precision'] for x in outputs]\n",
        "        epoch_precision = torch.stack(batch_precision).mean()  \n",
        "        batch_f1 = [x['val_f1'] for x in outputs]\n",
        "        epoch_f1 = torch.stack(batch_f1).mean()     \n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'val_recall': epoch_recall.item(), 'val_precision': epoch_precision.item(), 'val_f1': epoch_f1.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, val_prec: {:.4f}, val_recall: {:.4f}, val_f1: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc'], result['val_precision'], result['val_recall'], result['val_f1'] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDvRyALMjz0I"
      },
      "source": [
        "class CnnModel(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
        "            \n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "            \n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "          \n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(256*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 4))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzExm2aqj44O"
      },
      "source": [
        "model = CnnModel()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6d5mBJMkBc_"
      },
      "source": [
        "device = get_default_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1lH0I3SkEWW"
      },
      "source": [
        "train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
        "validation_dataloader = DeviceDataLoader(validation_dataloader, device)\n",
        "test_dataloader = DeviceDataLoader(test_dataloader, device)\n",
        "to_device(model, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MORxkuUEkHRh"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oYrMwNAkSRu"
      },
      "source": [
        "model = to_device(CnnModel(), device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN600OUckaUM"
      },
      "source": [
        "num_epochs = 200\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpiv28OtntoK"
      },
      "source": [
        "history_cnn = fit(num_epochs, lr, model, train_dataloader, validation_dataloader, opt_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR1XWw6HnxeQ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_accuracies(history, metric, ylabel, title):\n",
        "    accuracies = [x[metric] for x in history]\n",
        "    plt.plot(accuracies)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njln6hAUn-oM"
      },
      "source": [
        "plot_accuracies(history_cnn, 'val_acc', 'Validation Accuracy', 'Validation Accuracy vs. No. of epochs (CNN Model)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracies(history_cnn, 'val_f1', 'Validation F1 Score', 'Validation F1 Score vs. No. of epochs (CNN Model)')"
      ],
      "metadata": {
        "id": "7arBx9PLfxCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcNGdfTPqBtV"
      },
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-b')\n",
        "    plt.plot(val_losses, '-r')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs (CNN Model)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJVDxPWVqEpq"
      },
      "source": [
        "plot_losses(history_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_history_cnn = [evaluate(model, validation_dataloader)]"
      ],
      "metadata": {
        "id": "KA0xwZVWf9z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXfXplh2uiDX"
      },
      "source": [
        "test_history_cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3Vol0b2E5b"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/Computer Science and Engineering (CSE)/4th Year 2nd Semester/CSE4238 Soft Computing Lab/Assignment 01/one/cnn_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}